{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0b0253",
   "metadata": {},
   "source": [
    "## RDD:\n",
    "\n",
    "**RDD is the fundamental data structure of Spark. It is fault-tolerant (resilient) and immutable distributed collections of any type of objects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9539af",
   "metadata": {},
   "source": [
    "### Ways of creating an RDD\n",
    "\n",
    "There are basically 2 ways of creating RDDs\n",
    "\n",
    "1)Parallelizing an existing collection in your driver program\n",
    "    \n",
    "  To parallelize Collections in Driver program, Spark provides **SparkContext.parallelize ()** method. When spark parallelize\n",
    "  method is applied on a Collection (with elements), a new distributed data set is created with specified number of partitions\n",
    "  and the elements of the collection are copied to the distributed dataset (RDD).\n",
    "    \n",
    "2)Referencing a dataset in an external storage system, such as shared file system, HDFS HBase,or any dataSource offering\n",
    "  a Hadoop Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffd30d",
   "metadata": {},
   "source": [
    "#### Printing data distribution and partitions\n",
    "\n",
    "**You can get the number of Partitions using the getNumPartitions() Method**\n",
    "\n",
    "**Coalesce() and Repartition()**: \n",
    "**Coalesce** is used for reducing the number of partitions, could be used to change it to zero that means combine every data back to single.\n",
    "**Repartition** can be used to increase as well as decrease the number of partitions.\n",
    "\n",
    "**glom():** This function can be used to see the number of partitions in accordance with the **.collect()** method to see the elements in every partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8e503",
   "metadata": {},
   "source": [
    "### Other functions Which can be used for transformations and actions in RDDs\n",
    "\n",
    "1) **.count():** It is the count of elements in all the RDDs\n",
    "\n",
    "2) **.first():** It returns the first element of the first RDD created\n",
    "\n",
    "3) **.top():** It gives a list of first mentioned number of elements\n",
    "\n",
    "4) **.distinct():**\n",
    "\n",
    "5) **.map():** Map function assigns given value to every element of rdd and makes tuples\n",
    "\n",
    "6) **.filter():** This filters the data as per the given condition as argument\n",
    "\n",
    "7) **.flatmap():** Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.\n",
    "\n",
    "8) **.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d5c99",
   "metadata": {},
   "source": [
    "#### Advance functions for Transformations and actions\n",
    "\n",
    "1) **.union():** The PySpark union () function is used to combine two or more data frames having the same structure or schema\n",
    "\n",
    "2) **.intersection():** Finds common rows out of the given two dataframes\n",
    "\n",
    "3) **.takeSample (withReplacement, num, [seed]):**  function displays a random array of “num” elements where the seed is for the random number generator\n",
    "\n",
    "4) **takeOrdered(n,[ordering]):** it gives an ordered subset of n elements as per the mentioned order in the second parameter.\n",
    "\n",
    "5) **reduce():** It reduces the elements of the input RDD using the binary operator specified.\n",
    "\n",
    "6) **reduceByKey():** It receives key-value pairs (K, V) as an input, aggregates the values based on the key and generates a dataset of (K, V) pairs as an output.\n",
    "\n",
    "7) **sortByKey():** Sorts the RDD, which is assumed to consist of (key, value) pairs.\n",
    "\n",
    "8) **countByKey():** Count the number of elements for each key, and return the result to the master as a dictionary.\n",
    "\n",
    "9) **groupByKey():** Groups it as per the keys if in common\n",
    "\n",
    "10) **lookup(key):** this is used to look up the value of the given key in the parameter.\n",
    "\n",
    "11) **cache():** this is used for doing caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93309b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
